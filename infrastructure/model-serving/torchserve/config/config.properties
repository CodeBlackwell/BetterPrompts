# TorchServe Configuration for BetterPrompts

# Server Configuration
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082
number_of_netty_threads=32
job_queue_size=1000
model_store=/models
model_snapshot={"name":"startup.cfg","modelCount":1,"models":{"intent_classifier":{"1.0":{"defaultVersion":true,"marName":"intent_classifier.mar","minWorkers":2,"maxWorkers":4,"batchSize":8,"maxBatchDelay":100,"responseTimeout":60}}}}

# Batch Processing
batch_size=8
max_batch_delay=100

# Worker Configuration
default_workers_per_model=4
default_response_timeout=60
unregister_model_timeout=120

# CORS Configuration
cors_allowed_origin=*
cors_allowed_methods=GET, POST, PUT, OPTIONS
cors_allowed_headers=X-Custom-Header, Content-Type, Authorization

# SSL Configuration (for production)
# keystore=keystore.p12
# keystore_pass=changeit
# keystore_type=PKCS12
# certificate=certs/cert.pem
# private_key=certs/key.pem

# Model Configuration
load_models=intent_classifier.mar
models={\
  "intent_classifier": {\
    "1.0": {\
        "defaultVersion": true,\
        "marName": "intent_classifier.mar",\
        "minWorkers": 2,\
        "maxWorkers": 4,\
        "batchSize": 8,\
        "maxBatchDelay": 100,\
        "responseTimeout": 60\
    }\
  }\
}

# Logging
log4j_config_file=/config/log4j2.xml
vmargs=-Xmx4g -Xms2g -XX:+UseG1GC -XX:MaxGCPauseMillis=100

# Metrics
enable_metrics_api=true
metrics_format=prometheus
metrics_interval=60

# GPU Configuration (if available)
# number_of_gpu=1
# gpu_id=0

# Model Versioning
model_version_policy={\
  "latest": {\
    "num_versions": 2\
  },\
  "specific": {\
    "versions": [1, 2]\
  }\
}

# Security
# enable_model_api=false  # Disable in production
# allowed_urls=https://api.betterprompts.com/*

# Performance Tuning
# Enable dynamic batching
enable_dynamic_batching=true
# Preload models
preload_model=true
# Use shared memory for zero-copy
use_native_io=true